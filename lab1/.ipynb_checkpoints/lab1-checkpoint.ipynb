{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab1\n",
    "## Part 1\n",
    "### Indexing one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import pickle\n",
    "\n",
    "one_textfile = open('Selma/nils.txt', encoding='utf-8')\n",
    "text = one_textfile.read().lower()\n",
    "one_textfile.close()\n",
    "\n",
    "matches = re.finditer(r'[a-ö]+', text)\n",
    "\n",
    "index = {}\n",
    "\n",
    "for match in matches:\n",
    "    #print(\"%s: %s\" % (match.group(), match.start()))\n",
    "    if match.group() not in index.keys():\n",
    "        index[match.group()] = [match.start()]\n",
    "    else:\n",
    "        index[match.group()].append(match.start())\n",
    "\n",
    "#print(sorted(list(index.keys())))\n",
    "#print(index['gås'])\n",
    "\n",
    "stringie = \"\"\n",
    "for key in index.keys():\n",
    "    stringie_prep = key + \" \"\n",
    "    for val in index[key]:\n",
    "        stringie_prep += str(val) + \" \"\n",
    "    stringie += stringie_prep + \"\\n\"\n",
    "\n",
    "writefile = open('nils.p', 'wb')\n",
    "pickle.dump(index, writefile)\n",
    "writefile.close()\n",
    "\n",
    "opened_index = pickle.load(open('nils.p', 'rb'))\n",
    "#type(opened_index)\n",
    "#foo = opened_index.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the content of a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bannlyst.txt', 'gosta.txt', 'herrgard.txt', 'jerusalem.txt', 'kejsaren.txt', 'marbacka.txt', 'nils.txt', 'osynliga.txt', 'troll.txt']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def get_files(dir, suffix):\n",
    "        \"\"\"\n",
    "        Returns all the files in a folder ending with suffix\n",
    "        :param dir:\n",
    "        :param suffix:\n",
    "        :return: the list of file names\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        for file in os.listdir(dir):\n",
    "            if file.endswith(suffix):\n",
    "                files.append(file)\n",
    "        return files\n",
    "\n",
    "one_textfile = open('Selma/nils.txt', encoding='utf-8')\n",
    "text = one_textfile.read().lower()\n",
    "one_textfile.close()\n",
    "\n",
    "matches = re.finditer(r'[a-ö]+', text)\n",
    "\n",
    "index = {}\n",
    "\n",
    "for match in matches:\n",
    "    #print(\"%s: %s\" % (match.group(), match.start()))\n",
    "    if match.group() not in index.keys():\n",
    "        index[match.group()] = [match.start()]\n",
    "    else:\n",
    "        index[match.group()].append(match.start())\n",
    "\n",
    "#print(sorted(list(index.keys())))\n",
    "#print(index['gås'])\n",
    "\n",
    "stringie = \"\"\n",
    "for key in index.keys():\n",
    "    stringie_prep = key + \" \"\n",
    "    for val in index[key]:\n",
    "        stringie_prep += str(val) + \" \"\n",
    "    stringie += stringie_prep + \"\\n\"\n",
    "\n",
    "writefile = open('nils.p', 'wb')\n",
    "pickle.dump(index, writefile)\n",
    "writefile.close()\n",
    "    \n",
    "file_list = get_files('Selma', '.txt')\n",
    "print(file_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a master index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bannlyst.txt', 'gosta.txt', 'herrgard.txt', 'jerusalem.txt', 'kejsaren.txt', 'marbacka.txt', 'nils.txt', 'osynliga.txt', 'troll.txt']\n",
      "'samlar' in 'gosta.txt': [317119, 414300, 543686]\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def get_files(dir, suffix):\n",
    "        \"\"\"\n",
    "        Returns all the files in a folder ending with suffix\n",
    "        :param dir:\n",
    "        :param suffix:\n",
    "        :return: the list of file names\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        for file in os.listdir(dir):\n",
    "            if file.endswith(suffix):\n",
    "                files.append(file)\n",
    "        return files\n",
    "\n",
    "#print(sorted(list(index.keys())))\n",
    "#print(index['gås'])\n",
    "\n",
    "'''stringie = \"\"\n",
    "for key in index.keys():\n",
    "    stringie_prep = key + \" \"\n",
    "    for val in index[key]:\n",
    "        stringie_prep += str(val) + \" \"\n",
    "    stringie += stringie_prep + \"\\n\"\n",
    "\n",
    "writefile = open('nils.p', 'wb')\n",
    "pickle.dump(index, writefile)\n",
    "writefile.close()'''\n",
    "    \n",
    "file_list = get_files('Selma', '.txt')\n",
    "print(file_list)\n",
    "\n",
    "master_index = {}\n",
    "\n",
    "for filename in file_list:\n",
    "    file = open('Selma\\\\' + filename, encoding='utf-8')\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "\n",
    "    matches = re.finditer(r'[a-ö]+', text)\n",
    "\n",
    "    index = {}\n",
    "\n",
    "    for match in matches:\n",
    "        #print(\"%s: %s\" % (match.group(), match.start()))\n",
    "        if match.group() not in index.keys():\n",
    "            index[match.group()] = [match.start()]\n",
    "        else:\n",
    "            index[match.group()].append(match.start())\n",
    "    \n",
    "    master_index[filename] = index\n",
    "\n",
    "print('\\'samlar\\' in \\'gosta.txt\\':', master_index['gosta.txt']['samlar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Representing Documents with td-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bannlyst.txt', 'gosta.txt', 'herrgard.txt', 'jerusalem.txt', 'kejsaren.txt', 'marbacka.txt', 'nils.txt', 'osynliga.txt', 'troll.txt']\n",
      "tf_idfs['bannlyst.txt']['et'] = 6.284526537403352e-06\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'nils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-85d230de8c1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"tf_idfs['bannlyst.txt']['et'] = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_idfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bannlyst.txt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'et'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m print(tf_idfs['herrgard.txt']['känna'],\n\u001b[1;32m---> 91\u001b[1;33m \u001b[0mtf_idfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'herrgard.txt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nils'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[0mtf_idfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nils.txt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'känna'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[0mtf_idfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nils.txt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gås'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'nils'"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "\n",
    "def get_files(dir, suffix):\n",
    "        \"\"\"\n",
    "        Returns all the files in a folder ending with suffix\n",
    "        :param dir:\n",
    "        :param suffix:\n",
    "        :return: the list of file names\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        for file in os.listdir(dir):\n",
    "            if file.endswith(suffix):\n",
    "                files.append(file)\n",
    "        return files\n",
    "    \n",
    "def tfbs(word, document):\n",
    "    \"\"\"\n",
    "    Calculates term frequency of a word inside a document\n",
    "    This is done by dividing number of times word appears\n",
    "    in document by total number of words in document\n",
    "    \"\"\"\n",
    "    return \"bs\"\n",
    "    \n",
    "    \n",
    "file_list = get_files('Selma', '.txt')\n",
    "print(file_list)\n",
    "\n",
    "master_index = {}\n",
    "\n",
    "for filename in file_list:\n",
    "    file = open('Selma\\\\' + filename, encoding='utf-8')\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "\n",
    "    matches = re.finditer(r'[a-ö]+', text)\n",
    "\n",
    "    index = {}\n",
    "\n",
    "    for match in matches:\n",
    "        #print(\"%s: %s\" % (match.group(), match.start()))\n",
    "        if match.group() not in index.keys():\n",
    "            index[match.group()] = [match.start()]\n",
    "        else:\n",
    "            index[match.group()].append(match.start())\n",
    "    \n",
    "    master_index[filename] = index\n",
    "\n",
    "corpus_N = 0\n",
    "for wordlist in master_index.values():\n",
    "    corpus_N += len(list(wordlist.values()))\n",
    "\n",
    "try:\n",
    "    tf_idfs = pickle.load( open( \"tf_idfs_old.p\", \"rb\" ) )\n",
    "except:    \n",
    "    tf_idfs = {}\n",
    "\n",
    "    for document in file_list:#master_index.keys():\n",
    "        print('processing ' + document + \"...\")\n",
    "        d = 0.0\n",
    "        for leest in master_index[document].values():\n",
    "            d += len(list(leest))\n",
    "        print(d)\n",
    "        #d = len(list(master_index[document].keys())) #nog fel, bör räkna faktiskt antal ord och inte antal unika ord\n",
    "        tf_idfs[document] = {}\n",
    "        for tup in master_index[document].items():\n",
    "            #print(tup[0], tup[1])\n",
    "            tf = (len(list(tup[1])) / d)\n",
    "\n",
    "            doc_w_word = 0.0\n",
    "            for doc in file_list: #where tup[0] in list(master_index[document].keys()):\n",
    "                if tup[0] in list(master_index[doc].keys()):\n",
    "                    doc_w_word += 1.0\n",
    "            idf = 0.0\n",
    "            if doc_w_word:\n",
    "                idf = math.log10(len(file_list) / doc_w_word)\n",
    "\n",
    "            tf_idfs[document][tup[0]] = 0.0\n",
    "\n",
    "\n",
    "            tf_idfs[document][tup[0]] = tf * idf\n",
    "    writefile = open('tf_idfs.p', 'wb')\n",
    "    pickle.dump(tf_idfs, writefile)\n",
    "    writefile.close()\n",
    "\n",
    "#print(tf_idfs['bannlyst.txt']['känna'].items())\n",
    "print(r\"tf_idfs['bannlyst.txt']['et'] = \" + str(tf_idfs['bannlyst.txt']['et']))\n",
    "print(tf_idfs['herrgard.txt']['känna'],\n",
    "tf_idfs['herrgard.txt']['nils'],\n",
    "tf_idfs['nils.txt']['känna'],\n",
    "tf_idfs['nils.txt']['gås'],\n",
    "tf_idfs['nils.txt']['nils'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing bannlyst.txt...\n",
      "processing gosta.txt...\n",
      "processing herrgard.txt...\n",
      "processing jerusalem.txt...\n",
      "processing kejsaren.txt...\n",
      "processing marbacka.txt...\n",
      "processing nils.txt...\n",
      "processing osynliga.txt...\n",
      "processing troll.txt...\n",
      "obs: tf for nils in herrgard =  0.0\n",
      "dict_items([('bannlyst.txt', 0.0), ('gosta.txt', 0.0), ('herrgard.txt', 0.0), ('jerusalem.txt', 0.0), ('kejsaren.txt', 0.0), ('marbacka.txt', 0.0), ('nils.txt', 0.0), ('osynliga.txt', 0.0), ('troll.txt', 0.0)])\n",
      "tf_idfs['et']['bannlyst.txt'] = 6.284526537403352e-06\n",
      "0.0 0.0 0.0001012371942196493 9.801209641132888e-05 0.0\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "\n",
    "def get_files(dir, suffix):\n",
    "        \"\"\"\n",
    "        Returns all the files in a folder ending with suffix\n",
    "        :param dir:\n",
    "        :param suffix:\n",
    "        :return: the list of file names\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        for file in os.listdir(dir):\n",
    "            if file.endswith(suffix):\n",
    "                files.append(file)\n",
    "        return files\n",
    "    \n",
    "file_list = get_files('Selma', '.txt')\n",
    "\n",
    "master_index = {}\n",
    "d = {}\n",
    "all_corpus_words = set()\n",
    "\n",
    "for filename in file_list:\n",
    "    print('processing ' + filename + \"...\")\n",
    "    file = open('Selma\\\\' + filename, encoding='utf-8')\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "\n",
    "    matches = re.finditer(r'[a-ö]+', text)\n",
    "    \n",
    "    d[filename] = 0.0\n",
    "\n",
    "    for match in matches:\n",
    "        all_corpus_words.add(match.group())\n",
    "        \n",
    "        d[filename] += 1.0\n",
    "        \n",
    "        if match.group() not in master_index.keys():\n",
    "            master_index[match.group()] = {}\n",
    "        \n",
    "        if filename not in master_index[match.group()].keys():\n",
    "            master_index[match.group()][filename] = [match.start()]\n",
    "        else:\n",
    "            master_index[match.group()][filename].append(match.start())\n",
    "\n",
    "for filename in file_list:\n",
    "    for uniqueword in all_corpus_words:\n",
    "        if filename not in master_index[uniqueword].keys():\n",
    "            master_index[uniqueword][filename] = []\n",
    "\n",
    "for word in master_index.keys():\n",
    "    tf_idfs[word] = {}\n",
    "    doc_w_word = 0.0\n",
    "\n",
    "    for tup in master_index[word].items():\n",
    "        if word == 'nils' and tup[0] == 'herrgard.txt':\n",
    "            print(\"obs: tf for nils in herrgard = \", len(list(tup[1])) / d[tup[0]])\n",
    "        tf_idfs[word][tup[0]] = len(list(tup[1])) / d[tup[0]] #calculate tf\n",
    "        if len(list(tup[1])):\n",
    "            doc_w_word += 1.0\n",
    "\n",
    "    for key in tf_idfs[word].keys():\n",
    "        tf_idfs[word][key] *= math.log10(float(len(file_list)) / doc_w_word) #calculate idf and multiply tf with it, creating tf-idf\n",
    "\n",
    "print(tf_idfs['känna'].items())\n",
    "print(r\"tf_idfs['et']['bannlyst.txt'] = \" + str(tf_idfs['et']['bannlyst.txt']))\n",
    "print(tf_idfs['känna']['herrgard.txt'],\n",
    "tf_idfs['känna']['nils.txt'],\n",
    "tf_idfs['gås']['nils.txt'],\n",
    "tf_idfs['nils']['nils.txt'],\n",
    "tf_idfs['nils']['herrgard.txt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 3 - Comparing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building master_index..\n",
      "\tprocessing bannlyst.txt...\n",
      "\tprocessing gosta.txt...\n",
      "\tprocessing herrgard.txt...\n",
      "\tprocessing jerusalem.txt...\n",
      "\tprocessing kejsaren.txt...\n",
      "\tprocessing marbacka.txt...\n",
      "\tprocessing nils.txt...\n",
      "\tprocessing osynliga.txt...\n",
      "\tprocessing troll.txt...\n",
      "calculating cosine similarity..\n",
      "\tprocessing bannlyst.txt ...\n",
      "\tprocessing gosta.txt ...\n",
      "\tprocessing herrgard.txt ...\n",
      "\tprocessing jerusalem.txt ...\n",
      "\tprocessing kejsaren.txt ...\n",
      "\tprocessing marbacka.txt ...\n",
      "\tprocessing nils.txt ...\n",
      "\tprocessing osynliga.txt ...\n",
      "\tprocessing troll.txt ...\n",
      "\n",
      "\t\tbannly\tgosta.\therrga\tjerusa\tkejsar\tmarbac\tnils.t\tosynli\ttroll.\t\n",
      "bannlyst.txt\t1.00000\t0.00339\t0.00130\t0.00732\t0.00127\t0.00506\t0.00535\t0.00598\t0.00734\t\n",
      "gosta.txt\t0.00339\t1.00000\t0.00980\t0.00554\t0.00249\t0.01825\t0.01621\t0.03614\t0.02098\t\n",
      "herrgard.txt\t0.00130\t0.00980\t1.00000\t0.00762\t0.00053\t0.01161\t0.01415\t0.02125\t0.00399\t\n",
      "jerusalem.txt\t0.00732\t0.00554\t0.00762\t1.00000\t0.00121\t0.01280\t0.01309\t0.04184\t0.00707\t\n",
      "kejsaren.txt\t0.00127\t0.00249\t0.00053\t0.00121\t1.00000\t0.02607\t0.00258\t0.00324\t0.05208\t\n",
      "marbacka.txt\t0.00506\t0.01825\t0.01161\t0.01280\t0.02607\t1.00000\t0.03991\t0.04298\t0.02347\t\n",
      "nils.txt\t0.00535\t0.01621\t0.01415\t0.01309\t0.00258\t0.03991\t1.00000\t0.03213\t0.01939\t\n",
      "osynliga.txt\t0.00598\t0.03614\t0.02125\t0.04184\t0.00324\t0.04298\t0.03213\t1.00000\t0.02826\t\n",
      "troll.txt\t0.00734\t0.02098\t0.00399\t0.00707\t0.05208\t0.02347\t0.01939\t0.02826\t1.00000\t\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "\n",
    "def get_files(dir, suffix):\n",
    "        \"\"\"\n",
    "        Returns all the files in a folder ending with suffix\n",
    "        :param dir:\n",
    "        :param suffix:\n",
    "        :return: the list of file names\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        for file in os.listdir(dir):\n",
    "            if file.endswith(suffix):\n",
    "                files.append(file)\n",
    "        return files\n",
    "    \n",
    "file_list = get_files('Selma', '.txt')\n",
    "\n",
    "master_index = {}\n",
    "d = {}\n",
    "all_corpus_words = set()\n",
    "\n",
    "print(\"building master_index..\")\n",
    "for filename in file_list:\n",
    "    print('\\tprocessing ' + filename + \"...\")\n",
    "    file = open('Selma\\\\' + filename, encoding='utf-8')\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "\n",
    "    matches = re.finditer(r'[a-ö]+', text)\n",
    "    #matches = re.finditer(r'\\p{L}+', text)\n",
    "    \n",
    "    d[filename] = 0.0\n",
    "\n",
    "    for match in matches:\n",
    "        all_corpus_words.add(match.group())\n",
    "        \n",
    "        d[filename] += 1.0\n",
    "        \n",
    "        if match.group() not in master_index.keys():\n",
    "            master_index[match.group()] = {}\n",
    "        \n",
    "        if filename not in master_index[match.group()].keys():\n",
    "            master_index[match.group()][filename] = [match.start()]\n",
    "        else:\n",
    "            master_index[match.group()][filename].append(match.start())\n",
    "\n",
    "for filename in file_list:\n",
    "    for uniqueword in all_corpus_words:\n",
    "        if filename not in master_index[uniqueword].keys():\n",
    "            master_index[uniqueword][filename] = []\n",
    "            \n",
    "tf_idfs = {}\n",
    "for word in master_index.keys():\n",
    "    tf_idfs[word] = {}\n",
    "    doc_w_word = 0.0\n",
    "\n",
    "    for tup in master_index[word].items():\n",
    "        tf_idfs[word][tup[0]] = len(list(tup[1])) / d[tup[0]] #calculate tf\n",
    "        if len(list(tup[1])):\n",
    "            doc_w_word += 1.0\n",
    "\n",
    "    for key in tf_idfs[word].keys():\n",
    "        #calculate idf and multiply tf with it, creating tf-idf\n",
    "        tf_idfs[word][key] *= math.log10(float(len(file_list)) / doc_w_word) \n",
    "\n",
    "print('calculating cosine similarity..')\n",
    "cos_sim = []\n",
    "for filename in file_list: #cos(q,d)\n",
    "    print('\\tprocessing', filename, '...')\n",
    "    sim_list = []\n",
    "    for cmp_filename in file_list:\n",
    "        sum_dq = 0.0\n",
    "        sq_sum_d = 0.0\n",
    "        sq_sum_q = 0.0\n",
    "        for word in all_corpus_words:\n",
    "            sum_dq += tf_idfs[word][filename] * tf_idfs[word][cmp_filename]\n",
    "            sq_sum_d += math.pow(tf_idfs[word][filename], 2)\n",
    "            sq_sum_q += math.pow(tf_idfs[word][cmp_filename], 2)\n",
    "        #print(sq_sum_d, sq_sum_q)\n",
    "        sq_sum_d = math.sqrt(sq_sum_d)\n",
    "        sq_sum_q = math.sqrt(sq_sum_q)\n",
    "        sim_list.append(sum_dq / (sq_sum_d * sq_sum_q))\n",
    "    cos_sim.append(sim_list)\n",
    "\n",
    "print('\\n\\t\\t', end='')    \n",
    "for file in file_list:\n",
    "    print(file[:6], end='\\t')\n",
    "print()\n",
    "for i in range(len(cos_sim)):\n",
    "    print(file_list[i], end='\\t')\n",
    "    for j in range(len(cos_sim[i])):\n",
    "        print('%.5f' % cos_sim[i][j], end='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dict blabla =\n",
    "{\n",
    "    'troll.txt':\n",
    "        {\n",
    "            'troll.txt':cos_sim,\n",
    "            'nils.txt':cos_sim\n",
    "            ..\n",
    "        },\n",
    "    'nils.txt':\n",
    "        {\n",
    "            'troll.txt':cos_sim,\n",
    "            'nils.txt':cos_sim,\n",
    "            ..\n",
    "        }\n",
    "    ..\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
